{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/work/anaconda3/envs/tensorflow_gpuenv/lib/python36.zip', '/home/work/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages', '/home/work/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/dataset-1.1.0-py3.6.egg', '/home/work/.ipython', '/home/work/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/IPython/extensions', '/home/work/notebook', '/home/work/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/lib-dynload', '/home/work/anaconda3/envs/tensorflow_gpuenv/lib/python3.6', '/home/work/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/normality-1.0.0-py3.6.egg']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "tmp = set(sys.path)\n",
    "tmp.add(\"/home/work/notebook\")\n",
    "sys.path = list(tmp)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets\n",
    "#import urllib.request\n",
    "#dataset_prefix = 'http://s3.amazonaws.com/cornell-tech-sdl-openrec'\n",
    "#urllib.request.urlretrieve('%s/lastfm/lastfm_test.npy' % dataset_prefix, \n",
    "#                   'lastfm_test.npy')\n",
    "#urllib.request.urlretrieve('%s/lastfm/lastfm_train.npy' % dataset_prefix, \n",
    "#                   'lastfm_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import recsys.recommenders.recommender_base as recommender_base\n",
    "import recsys.modules.extractions.embedding_layer as embedding_layer\n",
    "import recsys.modules.interactions.rnn_softmax as rnn_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "recommender_base = imp.reload(recommender_base)\n",
    "embedding_layer = imp.reload(embedding_layer)\n",
    "rnn_softmax = imp.reload(rnn_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def VanillaRnnRec(batch_size, dim_item_embed, max_seq_len, total_items, num_units,\n",
    "        l2_reg_embed=None, init_model_dir=None,\n",
    "        save_model_dir='VanillaRnnRec/', train=True, serve=False):\n",
    "    \n",
    "    rec = recommender_base.Recommender(init_model_dir=init_model_dir,\n",
    "                      save_model_dir=save_model_dir, train=train, serve=serve)\n",
    "    \n",
    "    @rec.traingraph.inputgraph(outs=['seq_item_id', 'seq_len', 'label'])\n",
    "    def train_input_graph(subgraph):\n",
    "        subgraph['seq_item_id'] = tf.placeholder(tf.int32, \n",
    "                                      shape=[batch_size, max_seq_len],\n",
    "                                      name='seq_item_id')\n",
    "        subgraph['seq_len'] = tf.placeholder(tf.int32, \n",
    "                                      shape=[batch_size], \n",
    "                                      name='seq_len')\n",
    "        subgraph['label'] = tf.placeholder(tf.int32, \n",
    "                                      shape=[batch_size], \n",
    "                                      name='label')\n",
    "        subgraph.register_global_input_mapping({'seq_item_id': subgraph['seq_item_id'],\n",
    "                                                'seq_len': subgraph['seq_len'],\n",
    "                                                'label': subgraph['label']})\n",
    "        \n",
    "    @rec.servegraph.inputgraph(outs=['seq_item_id', 'seq_len'])\n",
    "    def serve_input_graph(subgraph):\n",
    "        subgraph['seq_item_id'] = tf.placeholder(tf.int32, \n",
    "                                      shape=[None, max_seq_len],\n",
    "                                      name='seq_item_id')\n",
    "        subgraph['seq_len'] = tf.placeholder(tf.int32, \n",
    "                                      shape=[None],\n",
    "                                      name='seq_len')\n",
    "        subgraph.register_global_input_mapping({'seq_item_id': subgraph['seq_item_id'],\n",
    "                                                'seq_len': subgraph['seq_len']})\n",
    "    \n",
    "    @rec.traingraph.itemgraph(ins=['seq_item_id'], outs=['seq_vec'])\n",
    "    @rec.servegraph.itemgraph(ins=['seq_item_id'], outs=['seq_vec'])\n",
    "    def item_graph(subgraph):\n",
    "        _, subgraph['seq_vec']= embedding_layer.apply(l2_reg=l2_reg_embed,\n",
    "                                      init='normal',\n",
    "                                      id_=subgraph['seq_item_id'],\n",
    "                                      shape=[total_items, dim_item_embed],\n",
    "                                      subgraph=subgraph,\n",
    "                                      scope='item')\n",
    "\n",
    "    @rec.traingraph.interactiongraph(ins=['seq_vec', 'seq_len', 'label'])\n",
    "    def train_interaction_graph(subgraph):\n",
    "        rnn_softmax.apply(\n",
    "            sequence=subgraph['seq_vec'], \n",
    "            seq_len=subgraph['seq_len'], \n",
    "            num_units=num_units, \n",
    "            cell_type='gru',\n",
    "            total_items=total_items, \n",
    "            label=subgraph['label'], \n",
    "            train=True, \n",
    "            subgraph=subgraph, \n",
    "            scope='RNNSoftmax')\n",
    "\n",
    "    @rec.servegraph.interactiongraph(ins=['seq_vec', 'seq_len'])\n",
    "    def serve_interaction_graph(subgraph):\n",
    "        rnn_softmax.apply(\n",
    "            sequence=subgraph['seq_vec'], \n",
    "            seq_len=subgraph['seq_len'],\n",
    "            num_units=num_units,\n",
    "            cell_type='gru',\n",
    "            total_items=total_items, \n",
    "            train=False, \n",
    "            subgraph=subgraph, \n",
    "            scope='RNNSoftmax')\n",
    "\n",
    "    @rec.traingraph.optimizergraph\n",
    "    def optimizer_graph(subgraph):\n",
    "        losses = tf.add_n(subgraph.get_global_losses())\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        subgraph.register_global_operation(optimizer.minimize(losses))\n",
    "\n",
    "    @rec.traingraph.connector\n",
    "    @rec.servegraph.connector\n",
    "    def connect(graph):\n",
    "        graph.itemgraph['seq_item_id'] = graph.inputgraph['seq_item_id']\n",
    "        graph.interactiongraph['seq_len'] = graph.inputgraph['seq_len']\n",
    "        graph.interactiongraph['seq_vec'] = graph.itemgraph['seq_vec']\n",
    "\n",
    "    @rec.traingraph.connector.extend\n",
    "    def train_connect(graph):\n",
    "        graph.interactiongraph['label'] = graph.inputgraph['label']\n",
    "\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992 14598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([(0, 1304, 1241478537), (0, 1036, 1241445250)],\n",
       "       dtype=[('user_id', '<i4'), ('item_id', '<i4'), ('ts', '<i4')]),\n",
       " array([(1, 282, 1240944095), (1, 282, 1240943945)],\n",
       "       dtype=[('user_id', '<i4'), ('item_id', '<i4'), ('ts', '<i4')]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "import recsys.dataset as dataset\n",
    "train_data = np.load('lastfm_train.npy')\n",
    "test_data = np.load('lastfm_test.npy')\n",
    "total_users = max(set(list(train_data['user_id']) + list(test_data['user_id']))) + 1\n",
    "total_items = max(set(list(train_data['item_id']) + list(test_data['item_id']))) + 1\n",
    "print(total_users, total_items)\n",
    "train_data[:2], test_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# datasets\n",
    "import recsys.dataset as dataset\n",
    "dataset = imp.reload(dataset)\n",
    "train_dataset = dataset.Dataset(train_data, total_users, total_items, \n",
    "                        sortby='ts', name='Train')\n",
    "test_dataset = dataset.Dataset(test_data, total_users, total_items, \n",
    "                       sortby='ts', name='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparamerters\n",
    "dim_item_embed = 50     # dimension of item embedding\n",
    "max_seq_len = 100       # the maxium length of user's listen history\n",
    "num_units = 32          # Number of units in the RNN model\n",
    "total_iter = int(1e3)   # iterations for training \n",
    "batch_size = 100        # training batch size\n",
    "eval_iter = 200         # iteration of evaluation\n",
    "save_iter = eval_iter   # iteration of saving model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = VanillaRnnRec(batch_size=batch_size, \n",
    "    dim_item_embed=dim_item_embed, \n",
    "    max_seq_len=max_seq_len, \n",
    "    total_items=train_dataset.total_items(), \n",
    "    num_units=num_units, \n",
    "    save_model_dir='VanillaRnnRec/', \n",
    "    train=True, serve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluators\n",
    "import recsys.metrics.auc as auc\n",
    "import recsys.metrics.ndcg as ndcg\n",
    "import recsys.metrics.recall as recall\n",
    "import recsys.metrics.precision as precision\n",
    "\n",
    "auc_evaluator = auc.AUC()\n",
    "ndcg_evaluator = ndcg.NDCG(ndcg_at=[100])\n",
    "recall_evaluator = recall.Recall(recall_at=[100, 200, 300, 400, 500])\n",
    "precision_evaluator = precision.Precision(precision_at=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sampler\n",
    "import recsys.samplers.temporal_sampler as temporal_sampler\n",
    "train_sampler = temporal_sampler.create_training_sampler(batch_size=batch_size, max_seq_len=max_seq_len, \n",
    "                                dataset=train_dataset, num_process=1)\n",
    "test_sampler = temporal_sampler.create_evaluation_sampler(dataset=test_dataset, \n",
    "                                         max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainer\n",
    "import recsys.model_trainer as model_trainer\n",
    "model_trainer = imp.reload(model_trainer)\n",
    "trainer = model_trainer.ModelTrainer(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[Training starts, total_iter: 1000, eval_iter: 200, save_iter: 200]\u001b[0m\n",
      "\u001b[31m[iter 200]\u001b[0m Model saved.\n",
      "\u001b[31m[iter 200]\u001b[0m loss: 9.145113\n",
      "\u001b[32m..(dataset: Test) evaluation\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from VanillaRnnRec/model.ckpt\n",
      "\u001b[32m..(dataset: Test)\u001b[0m AUC 0.727135027745427\n",
      "\u001b[32m..(dataset: Test)\u001b[0m NDCG 0.013805037637582434\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Recall 0.056666666666666664 0.11 0.13666666666666666 0.18 0.19\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Precision 0.0005666666666666667\n",
      "\u001b[31m[iter 400]\u001b[0m Model saved.\n",
      "\u001b[31m[iter 400]\u001b[0m loss: 8.827863\n",
      "\u001b[32m..(dataset: Test) evaluation\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from VanillaRnnRec/model.ckpt\n",
      "\u001b[32m..(dataset: Test)\u001b[0m AUC 0.7564718777831061\n",
      "\u001b[32m..(dataset: Test)\u001b[0m NDCG 0.012652879796065783\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Recall 0.06666666666666667 0.12333333333333334 0.16333333333333333 0.19 0.24\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Precision 0.0006666666666666668\n",
      "\u001b[31m[iter 600]\u001b[0m Model saved.\n",
      "\u001b[31m[iter 600]\u001b[0m loss: 8.742294\n",
      "\u001b[32m..(dataset: Test) evaluation\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from VanillaRnnRec/model.ckpt\n",
      "\u001b[32m..(dataset: Test)\u001b[0m AUC 0.7569059852481104\n",
      "\u001b[32m..(dataset: Test)\u001b[0m NDCG 0.012201307263516297\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Recall 0.06333333333333334 0.11 0.16333333333333333 0.19333333333333333 0.22\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Precision 0.0006333333333333333\n",
      "\u001b[31m[iter 800]\u001b[0m Model saved.\n",
      "\u001b[31m[iter 800]\u001b[0m loss: 8.698358\n",
      "\u001b[32m..(dataset: Test) evaluation\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from VanillaRnnRec/model.ckpt\n",
      "\u001b[32m..(dataset: Test)\u001b[0m AUC 0.7574905802562171\n",
      "\u001b[32m..(dataset: Test)\u001b[0m NDCG 0.011767993125831412\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Recall 0.056666666666666664 0.11666666666666667 0.16333333333333333 0.18333333333333332 0.22333333333333333\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Precision 0.0005666666666666667\n",
      "\u001b[31m[iter 1000]\u001b[0m Model saved.\n",
      "\u001b[31m[iter 1000]\u001b[0m loss: 8.648366\n",
      "\u001b[32m..(dataset: Test) evaluation\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from VanillaRnnRec/model.ckpt\n",
      "\u001b[32m..(dataset: Test)\u001b[0m AUC 0.7603521271494142\n",
      "\u001b[32m..(dataset: Test)\u001b[0m NDCG 0.010458230107463756\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Recall 0.056666666666666664 0.1 0.14333333333333334 0.17 0.20333333333333334\n",
      "\u001b[32m..(dataset: Test)\u001b[0m Precision 0.0005666666666666666\n"
     ]
    }
   ],
   "source": [
    "# train/test\n",
    "trainer.train(total_iter=total_iter, \n",
    "    eval_iter=eval_iter,\n",
    "    save_iter=save_iter,\n",
    "    train_sampler=train_sampler,\n",
    "    eval_samplers=[test_sampler], \n",
    "    evaluators=[auc_evaluator, ndcg_evaluator, recall_evaluator, precision_evaluator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
